{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_text=\"RAMAKRISHNA KRISHNA IS UP COMING DATA SCIENTIST AND THE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAMAKRISHNA KRISHNA IS UP COMING DATA SCIENTIST AND THE\n"
     ]
    }
   ],
   "source": [
    "print(nlp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: joblib in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\banda\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.0.12)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.11)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.4.0,>=8.3.4 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (8.3.4)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (1.1.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.5.1)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.4.1)\n",
      "Requirement already satisfied: typer<1.0.0,>=0.3.0 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (0.15.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.19.0 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.0.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.32.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (2.10.5)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.1.5)\n",
      "Requirement already satisfied: setuptools in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\banda\\appdata\\roaming\\python\\python311\\site-packages (from spacy) (24.2)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from spacy) (3.5.0)\n",
      "Requirement already satisfied: language-data>=1.2 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.3.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.27.2)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in c:\\users\\banda\\appdata\\roaming\\python\\python311\\site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.12.14)\n",
      "Requirement already satisfied: blis<1.3.0,>=1.2.0 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (1.2.0)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from thinc<8.4.0,>=8.3.4->spacy) (0.1.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\banda\\appdata\\roaming\\python\\python311\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.6)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer<1.0.0,>=0.3.0->spacy) (13.9.4)\n",
      "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.20.0)\n",
      "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->spacy) (3.0.2)\n",
      "Requirement already satisfied: marisa-trie>=1.1.0 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.1)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\banda\\appdata\\roaming\\python\\python311\\site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.19.1)\n",
      "Requirement already satisfied: wrapt in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import WordListCorpusReader\n",
    "import string\n",
    "import re\n",
    "from nltk import ne_chunk, pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\banda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\banda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\banda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\banda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\banda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\banda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\banda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\banda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK data\n",
    "nltk.download('averaged_perceptron_tagger') # for POS tagging\n",
    "nltk.download('averaged_perceptron_tagger_eng') # for specific to eng lang\n",
    "nltk.download('punkt') # Tokenizer models used for breaking down text into individual words.\n",
    "nltk.download('punkt_tab') # tokenizer for  tab-separated file \n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet') # for lemmintaization\n",
    "nltk.download('maxent_ne_chunker_tab') # for NER\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['C:\\\\Users\\\\banda/nltk_data', 'c:\\\\Users\\\\banda\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\nltk_data', 'c:\\\\Users\\\\banda\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\share\\\\nltk_data', 'c:\\\\Users\\\\banda\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data', 'C:\\\\Users\\\\banda\\\\AppData\\\\Roaming\\\\nltk_data', 'C:\\\\nltk_data', 'D:\\\\nltk_data', 'E:\\\\nltk_data', \"'c:\\\\\\\\Users\\\\\\\\banda\\\\\\\\AppData\\\\\\\\Local\\\\\\\\Programs\\\\\\\\Python\\\\\\\\Python311\\\\\\\\lib\\\\\\\\nltk_data'\"]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.data.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(r\"'c:\\\\Users\\\\banda\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python311\\\\lib\\\\nltk_data'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_text = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:  RAMAKRISHNA IS THE UP COMING DATA SCIENTIST\n",
      "Filtered Sentence:  RAMAKRISHNA COMING DATA SCIENTIST\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\banda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\banda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Define the sentence\n",
    "sentence = \"RAMAKRISHNA IS THE UP COMING DATA SCIENTIST\"\n",
    "\n",
    "\n",
    "# Step 3: Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Step 4: Remove stopwords from the tokenized sentence\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "# Step 5: Join the filtered tokens back into a sentence\n",
    "filtered_sentence = ' '.join(filtered_tokens)\n",
    "\n",
    "# Step 6: Display the original and the processed sentences\n",
    "print(\"Original Sentence: \", sentence)\n",
    "print(\"Filtered Sentence: \", filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:  ramakrishna is the up coming data scientist\n",
      "Filtered Sentence:  ramakrishna coming data scientist\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\banda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\banda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download the necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Step 1: Define the sentence\n",
    "sentence = \"RAMAKRISHNA IS THE UP COMING DATA SCIENTIST\"\n",
    "\n",
    "# Step 2: Convert the sentence to lowercase\n",
    "sentence = sentence.lower()\n",
    "\n",
    "# Step 3: Tokenize the sentence into words\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Step 4: Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Step 5: Remove stopwords from the tokenized sentence\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Step 6: Join the filtered tokens back into a sentence\n",
    "filtered_sentence = ' '.join(filtered_tokens)\n",
    "\n",
    "# Step 7: Display the original and the processed sentences\n",
    "print(\"Original Sentence: \", sentence)\n",
    "print(\"Filtered Sentence: \", filtered_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\banda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\banda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\banda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\banda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\banda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\banda\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence:  ramakrishna is the up coming data scientist\n",
      "Filtered Sentence (Stopwords removed):  ramakrishna coming data scientist\n",
      "Stemmed Sentence:  ramakrishna come data scientist\n",
      "Lemmatized Sentence:  ramakrishna come data scientist\n",
      "POS Tags:  [('ramakrishna', 'NN'), ('come', 'VBN'), ('data', 'NNS'), ('scientist', 'NN')]\n",
      "Named Entities:  [('ramakrishna', 'NN', 'O'), ('come', 'VBN', 'O'), ('data', 'NNS', 'O'), ('scientist', 'NN', 'O')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag, ne_chunk\n",
    "from nltk.chunk import tree2conlltags\n",
    "\n",
    "# Download the necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Step 1: Define the sentence\n",
    "sentence = \"RAMAKRISHNA IS THE UP COMING DATA SCIENTIST\"\n",
    "\n",
    "# Step 2: Convert the sentence to lowercase\n",
    "sentence = sentence.lower()\n",
    "\n",
    "# Step 3: Tokenize the sentence into words\n",
    "tokens = word_tokenize(sentence)\n",
    "\n",
    "# Step 4: Get the list of English stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Step 5: Remove stopwords from the tokenized sentence\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Step 6: Stemming (using Porter Stemmer)\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "# Step 7: Lemmatization (using WordNet Lemmatizer)\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
    "\n",
    "# Step 8: POS Tagging\n",
    "pos_tags = pos_tag(lemmatized_tokens)\n",
    "\n",
    "# Step 9: Named Entity Recognition (NER)\n",
    "ner_tree = ne_chunk(pos_tags)\n",
    "\n",
    "# Convert NER tree to a more readable format (Conll style)\n",
    "ner_entities = tree2conlltags(ner_tree)\n",
    "\n",
    "# Step 10: Join the filtered and processed tokens back into a sentence\n",
    "processed_sentence = ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Display results\n",
    "print(\"Original Sentence: \", sentence)\n",
    "print(\"Filtered Sentence (Stopwords removed): \", ' '.join(filtered_tokens))\n",
    "print(\"Stemmed Sentence: \", ' '.join(stemmed_tokens))\n",
    "print(\"Lemmatized Sentence: \", ' '.join(lemmatized_tokens))\n",
    "print(\"POS Tags: \", pos_tags)\n",
    "print(\"Named Entities: \", ner_entities)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[80], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ne_chunk\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchunk\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tree2conlltags\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.chunk import tree2conlltags\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Example Input Text\n",
    "text = [\"The weather is sunny and pleasant.\", \n",
    "        \"Natural Language Processing is fascinating!\", \n",
    "        \"Machine learning models can be trained for text classification.\",\n",
    "        \"Deep learning is part of machine learning but uses more complex models.\"]\n",
    "\n",
    "# Example Labels (These could be sentiment labels, topic labels, etc.)\n",
    "labels = [0, 1, 0, 1]  # For example, sentiment: 0 = negative, 1 = positive\n",
    "\n",
    "# Text Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing and removing punctuation\n",
    "    text_normalized = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text_normalized)\n",
    "    \n",
    "    # Remove punctuation from tokens (optional as already done in normalization)\n",
    "    filtered_tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens_without_stopwords = [word for word in filtered_tokens if word not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = [stemmer.stem(word) for word in tokens_without_stopwords]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(word) for word in tokens_without_stopwords]\n",
    "    \n",
    "    # Returning the preprocessed text as a string of lemmas (or stems if you prefer)\n",
    "    return ' '.join(lemmas)\n",
    "\n",
    "# Pre-process all texts\n",
    "processed_texts = [preprocess_text(t) for t in text]\n",
    "\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # Limiting features to top 1000 for simplicity\n",
    "X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "# Labels (For simplicity, I'm using a binary label, but this can be adapted for multi-class)\n",
    "y = labels\n",
    "\n",
    "# Train-test split (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Logistic Regression Model (you can change this to another model like SVM, Naive Bayes, etc.)\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Detailed classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Optionally, you can save the trained model using joblib or pickle\n",
    "import joblib\n",
    "joblib.dump(model, 'text_classification_model.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Obtaining dependency information for scikit-learn from https://files.pythonhosted.org/packages/a1/a6/c5b78606743a1f28eae8f11973de6613a5ee87366796583fb74c67d54939/scikit_learn-1.6.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading scikit_learn-1.6.1-cp311-cp311-win_amd64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (2.0.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Obtaining dependency information for scipy>=1.6.0 from https://files.pythonhosted.org/packages/af/25/caa430865749d504271757cafd24066d596217e83326155993980bc22f97/scipy-1.15.1-cp311-cp311-win_amd64.whl.metadata\n",
      "  Downloading scipy-1.15.1-cp311-cp311-win_amd64.whl.metadata (60 kB)\n",
      "     ---------------------------------------- 0.0/60.8 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/60.8 kB ? eta -:--:--\n",
      "     ------------------- ------------------ 30.7/60.8 kB 435.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 60.8/60.8 kB 537.5 kB/s eta 0:00:00\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\banda\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Obtaining dependency information for threadpoolctl>=3.1.0 from https://files.pythonhosted.org/packages/4b/2c/ffbf7a134b9ab11a67b0cf0726453cedd9c5043a4fe7a35d1cefa9a1bcfb/threadpoolctl-3.5.0-py3-none-any.whl.metadata\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.6.1-cp311-cp311-win_amd64.whl (11.1 MB)\n",
      "   ---------------------------------------- 0.0/11.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/11.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/11.1 MB 1.1 MB/s eta 0:00:11\n",
      "    --------------------------------------- 0.2/11.1 MB 1.8 MB/s eta 0:00:07\n",
      "   - -------------------------------------- 0.4/11.1 MB 2.2 MB/s eta 0:00:05\n",
      "   - -------------------------------------- 0.5/11.1 MB 2.5 MB/s eta 0:00:05\n",
      "   --- ------------------------------------ 0.9/11.1 MB 3.5 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 1.0/11.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.2/11.1 MB 3.6 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.3/11.1 MB 3.5 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.4/11.1 MB 3.3 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.4/11.1 MB 3.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.5/11.1 MB 3.0 MB/s eta 0:00:04\n",
      "   ----- ---------------------------------- 1.6/11.1 MB 3.0 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 2.2/11.1 MB 3.6 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.3/11.1 MB 3.6 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.5/11.1 MB 3.7 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.6/11.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.9/11.1 MB 3.8 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.9/11.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.9/11.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.9/11.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.9/11.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.9/11.1 MB 3.7 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 3.0/11.1 MB 2.9 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.2/11.1 MB 3.0 MB/s eta 0:00:03\n",
      "   --------------- ------------------------ 4.3/11.1 MB 3.8 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 4.6/11.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.8/11.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.8/11.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.8/11.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 4.8/11.1 MB 4.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.6/11.1 MB 4.1 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 5.8/11.1 MB 4.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 5.9/11.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.2/11.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.2/11.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.3/11.1 MB 4.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.5/11.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.7/11.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.7/11.1 MB 3.9 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.8/11.1 MB 3.8 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 6.8/11.1 MB 3.7 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.0/11.1 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.2/11.1 MB 3.8 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.3/11.1 MB 3.7 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 7.5/11.1 MB 3.7 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.7/11.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.9/11.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.1/11.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.2/11.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.3/11.1 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.4/11.1 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 8.5/11.1 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 8.9/11.1 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.0/11.1 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.1/11.1 MB 3.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 9.1/11.1 MB 3.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.2/11.1 MB 3.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.3/11.1 MB 3.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.5/11.1 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.5/11.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 9.6/11.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.8/11.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.8/11.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.1/11.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 10.3/11.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.3/11.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.5/11.1 MB 3.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.6/11.1 MB 3.5 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.8/11.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.9/11.1 MB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.1/11.1 MB 3.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.1/11.1 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading scipy-1.15.1-cp311-cp311-win_amd64.whl (43.9 MB)\n",
      "   ---------------------------------------- 0.0/43.9 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/43.9 MB 3.6 MB/s eta 0:00:13\n",
      "   ---------------------------------------- 0.2/43.9 MB 3.0 MB/s eta 0:00:15\n",
      "   ---------------------------------------- 0.4/43.9 MB 4.0 MB/s eta 0:00:11\n",
      "   ---------------------------------------- 0.5/43.9 MB 3.2 MB/s eta 0:00:14\n",
      "    --------------------------------------- 0.8/43.9 MB 3.7 MB/s eta 0:00:12\n",
      "    --------------------------------------- 1.0/43.9 MB 3.9 MB/s eta 0:00:11\n",
      "    --------------------------------------- 1.1/43.9 MB 4.0 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 1.4/43.9 MB 4.0 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 1.5/43.9 MB 4.1 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 1.7/43.9 MB 3.9 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 1.9/43.9 MB 4.0 MB/s eta 0:00:11\n",
      "   - -------------------------------------- 2.2/43.9 MB 4.1 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 2.4/43.9 MB 4.1 MB/s eta 0:00:11\n",
      "   -- ------------------------------------- 2.7/43.9 MB 4.4 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 2.8/43.9 MB 4.2 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 3.1/43.9 MB 4.4 MB/s eta 0:00:10\n",
      "   -- ------------------------------------- 3.3/43.9 MB 4.4 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 3.5/43.9 MB 4.4 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 3.6/43.9 MB 4.4 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 3.8/43.9 MB 4.4 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 4.1/43.9 MB 4.4 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 4.2/43.9 MB 4.3 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 4.4/43.9 MB 4.3 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 4.5/43.9 MB 4.2 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 4.8/43.9 MB 4.4 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 5.1/43.9 MB 4.5 MB/s eta 0:00:09\n",
      "   ---- ----------------------------------- 5.4/43.9 MB 4.4 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 5.6/43.9 MB 4.5 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 5.9/43.9 MB 4.6 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 6.1/43.9 MB 4.5 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 6.3/43.9 MB 4.5 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 6.7/43.9 MB 4.6 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 6.9/43.9 MB 4.7 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 7.1/43.9 MB 4.7 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 7.3/43.9 MB 4.7 MB/s eta 0:00:08\n",
      "   ------ --------------------------------- 7.6/43.9 MB 4.7 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 7.8/43.9 MB 4.7 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 8.1/43.9 MB 4.8 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 8.3/43.9 MB 4.8 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 8.5/43.9 MB 4.8 MB/s eta 0:00:08\n",
      "   ------- -------------------------------- 8.8/43.9 MB 4.8 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 9.0/43.9 MB 4.8 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 9.3/43.9 MB 4.8 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 9.4/43.9 MB 4.8 MB/s eta 0:00:08\n",
      "   -------- ------------------------------- 9.7/43.9 MB 4.8 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 9.9/43.9 MB 4.8 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 10.1/43.9 MB 4.8 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 10.2/43.9 MB 4.7 MB/s eta 0:00:08\n",
      "   --------- ------------------------------ 10.5/43.9 MB 4.8 MB/s eta 0:00:07\n",
      "   --------- ------------------------------ 10.7/43.9 MB 4.9 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 11.0/43.9 MB 4.9 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 11.2/43.9 MB 4.9 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 11.5/43.9 MB 5.0 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 11.6/43.9 MB 4.9 MB/s eta 0:00:07\n",
      "   ---------- ----------------------------- 12.0/43.9 MB 5.1 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 12.1/43.9 MB 5.1 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 12.5/43.9 MB 5.2 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 12.7/43.9 MB 5.1 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 12.7/43.9 MB 5.1 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 12.9/43.9 MB 5.0 MB/s eta 0:00:07\n",
      "   ----------- ---------------------------- 13.1/43.9 MB 5.0 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 13.3/43.9 MB 5.0 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 13.6/43.9 MB 5.0 MB/s eta 0:00:07\n",
      "   ------------ --------------------------- 13.7/43.9 MB 5.0 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 14.0/43.9 MB 5.1 MB/s eta 0:00:06\n",
      "   ------------ --------------------------- 14.2/43.9 MB 5.0 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 14.6/43.9 MB 5.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 14.8/43.9 MB 5.3 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 15.0/43.9 MB 5.2 MB/s eta 0:00:06\n",
      "   ------------- -------------------------- 15.3/43.9 MB 5.2 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 16.0/43.9 MB 5.3 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 16.2/43.9 MB 5.3 MB/s eta 0:00:06\n",
      "   -------------- ------------------------- 16.4/43.9 MB 5.3 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 16.8/43.9 MB 5.4 MB/s eta 0:00:06\n",
      "   --------------- ------------------------ 17.1/43.9 MB 5.5 MB/s eta 0:00:05\n",
      "   --------------- ------------------------ 17.3/43.9 MB 5.4 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 17.6/43.9 MB 5.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 17.9/43.9 MB 5.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 18.3/43.9 MB 5.5 MB/s eta 0:00:05\n",
      "   ---------------- ----------------------- 18.6/43.9 MB 5.5 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 19.0/43.9 MB 5.6 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 19.2/43.9 MB 5.6 MB/s eta 0:00:05\n",
      "   ----------------- ---------------------- 19.5/43.9 MB 5.6 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 19.8/43.9 MB 5.8 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 20.1/43.9 MB 5.7 MB/s eta 0:00:05\n",
      "   ------------------ --------------------- 20.5/43.9 MB 5.9 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 20.9/43.9 MB 6.0 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 21.0/43.9 MB 6.0 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 21.2/43.9 MB 6.0 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 21.4/43.9 MB 6.0 MB/s eta 0:00:04\n",
      "   ------------------- -------------------- 21.9/43.9 MB 6.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 22.1/43.9 MB 6.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 22.4/43.9 MB 6.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 22.7/43.9 MB 6.1 MB/s eta 0:00:04\n",
      "   -------------------- ------------------- 22.9/43.9 MB 6.0 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 23.2/43.9 MB 6.4 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 23.4/43.9 MB 6.3 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 23.5/43.9 MB 6.3 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 23.5/43.9 MB 6.1 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 23.7/43.9 MB 6.1 MB/s eta 0:00:04\n",
      "   --------------------- ------------------ 24.0/43.9 MB 6.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 24.3/43.9 MB 6.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 24.6/43.9 MB 6.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 24.6/43.9 MB 6.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 24.6/43.9 MB 6.2 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 24.9/43.9 MB 5.8 MB/s eta 0:00:04\n",
      "   ---------------------- ----------------- 25.2/43.9 MB 5.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 25.5/43.9 MB 5.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 25.7/43.9 MB 5.9 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 26.1/43.9 MB 5.8 MB/s eta 0:00:04\n",
      "   ----------------------- ---------------- 26.3/43.9 MB 5.8 MB/s eta 0:00:04\n",
      "   ------------------------ --------------- 26.8/43.9 MB 6.0 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 27.0/43.9 MB 5.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 27.0/43.9 MB 5.9 MB/s eta 0:00:03\n",
      "   ------------------------ --------------- 27.3/43.9 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 27.5/43.9 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 27.7/43.9 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 28.0/43.9 MB 5.7 MB/s eta 0:00:03\n",
      "   ------------------------- -------------- 28.3/43.9 MB 5.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 28.6/43.9 MB 5.8 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 28.9/43.9 MB 5.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 29.2/43.9 MB 5.7 MB/s eta 0:00:03\n",
      "   -------------------------- ------------- 29.4/43.9 MB 5.7 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 29.7/43.9 MB 5.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 30.0/43.9 MB 5.7 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 30.4/43.9 MB 5.8 MB/s eta 0:00:03\n",
      "   --------------------------- ------------ 30.6/43.9 MB 5.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 30.8/43.9 MB 5.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 31.1/43.9 MB 5.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 31.5/43.9 MB 5.7 MB/s eta 0:00:03\n",
      "   ---------------------------- ----------- 31.8/43.9 MB 5.8 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 32.1/43.9 MB 5.8 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 32.2/43.9 MB 5.8 MB/s eta 0:00:03\n",
      "   ----------------------------- ---------- 32.5/43.9 MB 5.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 32.7/43.9 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 33.1/43.9 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 33.1/43.9 MB 5.8 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 33.2/43.9 MB 5.6 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 33.4/43.9 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 33.6/43.9 MB 5.5 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 34.0/43.9 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 34.2/43.9 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 34.5/43.9 MB 5.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 34.8/43.9 MB 5.6 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 35.1/43.9 MB 6.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 35.4/43.9 MB 6.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 35.6/43.9 MB 6.0 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 36.0/43.9 MB 6.1 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 36.2/43.9 MB 6.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 36.4/43.9 MB 6.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 36.8/43.9 MB 6.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 37.0/43.9 MB 6.0 MB/s eta 0:00:02\n",
      "   --------------------------------- ------ 37.2/43.9 MB 6.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 37.3/43.9 MB 6.0 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 37.7/43.9 MB 6.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 37.9/43.9 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 38.1/43.9 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 38.4/43.9 MB 6.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 38.8/43.9 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 39.1/43.9 MB 6.0 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 39.3/43.9 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 39.6/43.9 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 39.9/43.9 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.0/43.9 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 40.4/43.9 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 40.7/43.9 MB 5.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.1/43.9 MB 6.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 41.3/43.9 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 41.7/43.9 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.0/43.9 MB 6.0 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.4/43.9 MB 6.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.5/43.9 MB 6.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------  42.9/43.9 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.2/43.9 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.5/43.9 MB 6.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.9/43.9 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.9/43.9 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.9/43.9 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.9/43.9 MB 6.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 43.9/43.9 MB 5.7 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.6.1 scipy-1.15.1 threadpoolctl-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['weather', 'sunny', 'pleasant']\n",
      "['natural', 'language', 'processing', 'fascinating']\n",
      "['machine', 'learning', 'model', 'trained', 'text', 'classification']\n",
      "['deep', 'learning', 'part', 'machine', 'learning', 'us', 'complex', 'model']\n",
      "-----------------------------------------------\n",
      "['weather sunny pleasant', 'natural language processing fascinating', 'machine learning model trained text classification', 'deep learning part machine learning us complex model']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.chunk import tree2conlltags\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# # Download necessary NLTK resources\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# Example Input Text\n",
    "text = [\"The weather is sunny and pleasant.\", \n",
    "        \"Natural Language Processing is fascinating!\", \n",
    "        \"Machine learning models can be trained for text classification.\",\n",
    "        \"Deep learning is part of machine learning but uses more complex models.\"]\n",
    "\n",
    "# Example Labels (These could be sentiment labels, topic labels, etc.)\n",
    "labels = [0, 1, 0, 1]  # For example, sentiment: 0 = negative, 1 = positive\n",
    "\n",
    "# Text Preprocessing Function\n",
    "def preprocess_text(text):\n",
    "    # Lowercasing and removing punctuation\n",
    "    text_normalized = text.lower().translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text_normalized)\n",
    "    \n",
    "    # Remove punctuation from tokens (optional as already done in normalization)\n",
    "    filtered_tokens = [token for token in tokens if token not in string.punctuation]\n",
    "    \n",
    "    # Removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens_without_stopwords = [word for word in filtered_tokens if word not in stop_words]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    stems = [stemmer.stem(word) for word in tokens_without_stopwords]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = [lemmatizer.lemmatize(word) for word in tokens_without_stopwords]\n",
    "    print(lemmas)\n",
    "    # Returning the preprocessed text as a string of lemmas (or stems if you prefer)\n",
    "\n",
    "    a = ' '.join(lemmas)\n",
    "    #return ' '.join(lemmas)\n",
    "    return a\n",
    "\n",
    "# Pre-process all texts\n",
    "processed_texts = [preprocess_text(t) for t in text]\n",
    "print(\"-----------------------------------------------\")\n",
    "print(processed_texts)\n",
    "# TF-IDF Vectorization\n",
    "vectorizer = TfidfVectorizer(max_features=1000)  # Limiting features to top 1000 for simplicity\n",
    "X = vectorizer.fit_transform(processed_texts)\n",
    "\n",
    "# Labels (For simplicity, I'm using a binary label, but this can be adapted for multi-class)\n",
    "y = labels\n",
    "\n",
    "# Train-test split (80% training, 20% testing)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\n",
    "\n",
    "# Logistic Regression Model (you can change this to another model like SVM, Naive Bayes, etc.)\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#accuracy_score(X_train,y_train)\n",
    "\n",
    "\n",
    "# # Model Evaluation\n",
    "# accuracy = accuracy_score(y_test, y_pred)\n",
    "# print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# # Detailed classification report\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_train,y_train)\n",
    "#model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GPT2LMHeadModel, GPT2Tokenizer\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "\n",
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt')\n",
    "\n",
    "# 1. Text Preprocessing\n",
    "def preprocess_text(text):\n",
    "    # Convert text to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove any unwanted characters (e.g., special characters, digits)\n",
    "    text = re.sub(r\"[^a-zA-Z\\s]\", \"\", text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "\n",
    "    # Join tokens back to a string\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# 2. Load GPT-2 model and tokenizer\n",
    "model_name = \"gpt2\"  # You can use other models like \"gpt2-medium\", \"gpt2-large\", etc.\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# 3. Function for text prediction\n",
    "def predict_next_sequence(input_text, max_length=50):\n",
    "    # Preprocess the input text\n",
    "    input_text = preprocess_text(input_text)\n",
    "    \n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(inputs, max_length=max_length, num_return_sequences=1, no_repeat_ngram_size=2, top_p=0.92, top_k=50)\n",
    "\n",
    "    # Decode the generated text\n",
    "    predicted_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return predicted_text\n",
    "\n",
    "# Example usage\n",
    "input_text = \"Once upon a time, in a land far, far away\"\n",
    "predicted_text = predict_next_sequence(input_text)\n",
    "print(\"Predicted Text: \", predicted_text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
