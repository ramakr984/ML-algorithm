{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "119\n"
     ]
    }
   ],
   "source": [
    "# Loading an Image\n",
    "import cv2\n",
    "\n",
    "# Load an image from file\n",
    "image = cv2.imread(r'H:\\01_Training\\Batch51\\05_DeepLearning\\OpenCV\\zebra.jpg')\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow('Image', image)\n",
    "key = cv2.waitKey(0)  # Wait for a key press\n",
    "cv2.destroyAllWindows() # close active image windows\n",
    "print(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Load the pre-trained face detection model\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Start the webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the frame to grayscale\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "    # Draw rectangles around the detected faces\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow('Face Detection', frame)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Start the webcam\n",
    "cap = cv2.VideoCapture(1)\n",
    "\n",
    "# Define the color range to track (HSV format for blue color) HSV = Hue, Saturation, and Value\n",
    "lower_blue = np.array([100, 150, 0])\n",
    "upper_blue = np.array([140, 255, 255])\n",
    "\n",
    "# Define the color range to track (HSV format for green color)\n",
    "# lower_green = np.array([35, 150, 50])    # Lower bound for green\n",
    "# upper_green = np.array([85, 255, 255])   # Upper bound for green\n",
    "\n",
    "\n",
    "while True:\n",
    "    # Capture frame-by-frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Convert the frame to HSV color space\n",
    "    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "    # Create a mask for blue color\n",
    "    # Pixels that fall within the specified color range are set to 255 (white).\n",
    "    # Pixels that fall outside the color range are set to 0 (black).\n",
    "    mask = cv2.inRange(hsv, lower_blue, upper_blue)\n",
    "\n",
    "    # Apply the mask to the original frame\n",
    "    # The areas matching the target color (blue) will remain as they are in the original image (frame).\n",
    "    # The areas not matching the target color will be set to black (because the corresponding pixels in the mask are 0).\n",
    "    result = cv2.bitwise_and(frame, frame, mask=mask)\n",
    "\n",
    "    # Display the frames\n",
    "    cv2.imshow('Original Frame', frame)\n",
    "    cv2.imshow('Mask', mask)\n",
    "    cv2.imshow('Tracked Color', result)\n",
    "\n",
    "    # Break the loop on 'q' key press\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the webcam and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO model\n",
    "net = cv2.dnn.readNet('yolov3.weights', 'yolov3.cfg')\n",
    "\n",
    "# Load the COCO dataset class labels\n",
    "with open('coco.names', 'r') as f:\n",
    "    classes = f.read().strip().split('\\n')\n",
    "\n",
    "# Load an image\n",
    "image = cv2.imread('zebra+and+elephant.png')\n",
    "height, width = image.shape[:2]\n",
    "\n",
    "# Convert the image to a blob\n",
    "blob = cv2.dnn.blobFromImage(image, 1/255.0, (416, 416), swapRB=True, crop=False)\n",
    "net.setInput(blob)\n",
    "\n",
    "# Get output layer names\n",
    "output_layers = net.getUnconnectedOutLayersNames()\n",
    "\n",
    "# Forward pass\n",
    "detections = net.forward(output_layers)\n",
    "\n",
    "# Draw bounding boxes\n",
    "for detection in detections:\n",
    "    for obj in detection:\n",
    "        scores = obj[5:]\n",
    "        class_id = np.argmax(scores)\n",
    "        confidence = scores[class_id]\n",
    "\n",
    "        if confidence > 0.5:\n",
    "            # Object detected\n",
    "            center_x, center_y, w, h = (obj[0:4] * np.array([width, height, width, height])).astype('int')\n",
    "\n",
    "            # Get top-left corner\n",
    "            x = int(center_x - w / 2)\n",
    "            y = int(center_y - h / 2)\n",
    "\n",
    "            # Draw rectangle and label\n",
    "            cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            cv2.putText(image, f'{classes[class_id]}: {confidence:.2f}', (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "# Display the image\n",
    "cv2.imshow('YOLO Object Detection', image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# Load YOLO\n",
    "net = cv2.dnn.readNet(\"yolov3.weights\", \"yolov3.cfg\")  # Load the YOLO model\n",
    "layer_names = net.getLayerNames()\n",
    "output_layers = [layer_names[i - 1] for i in net.getUnconnectedOutLayers()]\n",
    "\n",
    "# Load class labels (COCO dataset classes)\n",
    "with open(\"coco.names\", \"r\") as f:\n",
    "    classes = [line.strip() for line in f.readlines()]\n",
    "\n",
    "# Set up video capture (using the webcam)\n",
    "cap = cv2.VideoCapture(1)  # Use '0' for webcam, or provide a video path for a file\n",
    "\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Resize the frame to fit the model (YOLO expects 416x416 size)\n",
    "    height, width, channels = frame.shape\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "\n",
    "    # Feed the blob into the YOLO network\n",
    "    net.setInput(blob)\n",
    "    outputs = net.forward(output_layers)\n",
    "\n",
    "    # Process detections\n",
    "    class_ids = []\n",
    "    confidences = []\n",
    "    boxes = []\n",
    "\n",
    "    for output in outputs:\n",
    "        for detection in output:\n",
    "            scores = detection[5:]  # Get the scores (confidence) for each class\n",
    "            class_id = np.argmax(scores)  # Class with the highest confidence score\n",
    "            confidence = scores[class_id]\n",
    "            \n",
    "            if confidence > 0.5:  # If confidence is above threshold\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "\n",
    "                # Draw rectangle around the detected object\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Non-maxima suppression to remove redundant overlapping boxes\n",
    "    indices = cv2.dnn.NMSBoxes(boxes, confidences, score_threshold=0.5, nms_threshold=0.4)\n",
    "\n",
    "    # Draw bounding boxes and labels on the frame\n",
    "    if len(indices) > 0:\n",
    "        for i in indices.flatten():\n",
    "            x, y, w, h = boxes[i]\n",
    "            label = str(classes[class_ids[i]])\n",
    "            confidence = confidences[i]\n",
    "            \n",
    "            cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "            cv2.putText(frame, f\"{label} {confidence:.2f}\", (x, y - 10),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the frame with detected objects\n",
    "    cv2.imshow(\"Real-Time Object Detection (YOLO)\", frame)\n",
    "\n",
    "    # Break loop if 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
